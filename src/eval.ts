import { Eval, currentSpan } from "braintrust";
import { Factuality } from "autoevals";

import { getTestCasesForServer } from "./suite.js";
import { createRunner, resolveModel } from "./agent/index.js";
import { scoreCompleteness } from "./scorers/completeness.js";
import { scoreEfficiency } from "./scorers/efficiency.js";
import type { SuiteConfig, TestCase } from "./suite.js";

export interface RunEvalsOptions {
  tags?: string[];
}

// Collect tagging promises so we can ensure they complete before process exit
const tagPromises: Promise<void>[] = [];

async function tagExperiment(experimentId: string, tags: string[]) {
  const apiUrl = process.env.BRAINTRUST_API_URL ?? "https://api.braintrust.dev";
  const res = await fetch(`${apiUrl}/v1/experiment/${experimentId}`, {
    method: "PATCH",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.BRAINTRUST_API_KEY}`,
    },
    body: JSON.stringify({ tags }),
  });
  if (!res.ok) {
    console.error(
      `Failed to tag experiment ${experimentId}: ${res.status} ${await res.text()}`,
    );
  }
}

process.on("beforeExit", async () => {
  await Promise.allSettled(tagPromises);
});

export function runEvals(suite: SuiteConfig, options?: RunEvalsOptions): void {
  const cliTags: string[] = options?.tags ?? [];

  for (const server of suite.servers) {
    const modelAliases = server.models ?? ["opus"];
    const testCases = getTestCasesForServer(
      suite.testCases,
      server.capabilities,
    );
    const serverTags: string[] = server.tags ?? [];

    for (const alias of modelAliases) {
      const modelConfig = resolveModel(alias);
      const runner = createRunner(modelConfig);
      const experimentName = `${server.id}-${modelConfig.alias}`;
      const experimentTags = [...new Set([...serverTags, ...cliTags])];
      let taggedExperiment = false;

      Eval(suite.projectName, {
        experimentName,
        maxConcurrency: 2,
        metadata: {
          name: experimentName,
          server: server.id,
          model: modelConfig.alias,
          provider: modelConfig.provider,
          modelId: modelConfig.modelId,
          approach: "e2e",
          mode: server.mode,
          serverTags,
          cliTags,
        },
        data: () =>
          testCases.map((tc) => ({
            input: {
              prompt: tc.prompt,
              testCaseId: tc.id,
              tags: tc.tags,
            },
            tags: tc.tags,
            expected: tc.expected.description,
            metadata: {
              testCaseId: tc.id,
              serverId: server.id,
              model: modelConfig.alias,
              tags: tc.tags,
            },
          })),
        task: async (input, { span }) => {
          // Tag the experiment once via REST API on the first task invocation
          if (!taggedExperiment && experimentTags.length > 0) {
            taggedExperiment = true;
            const parentId = await (span as any).parentObjectId?.get?.();
            if (parentId) {
              tagPromises.push(tagExperiment(parentId, experimentTags));
            }
          }

          const result = await runner.run(input.prompt, server, {
            systemPrompt: suite.systemPrompt,
            model: modelConfig,
          });

          // Log raw metrics to Braintrust
          currentSpan().log({
            metrics: {
              inputTokens: result.inputTokens,
              outputTokens: result.outputTokens,
              totalTokens: result.inputTokens + result.outputTokens,
              tool_calls: result.toolCalls.length,
              turnCount: result.turnCount,
              wallClockMs: result.wallClockMs,
              costUsd: result.costUsd,
            },
            metadata: {
              model: modelConfig.alias,
              modelId: result.model,
              provider: modelConfig.provider,
            },
          });

          // Return structured output for scorers
          return JSON.stringify({
            finalText: result.finalText,
            toolCalls: result.toolCalls,
            turnCount: result.turnCount,
            totalTokens: result.inputTokens + result.outputTokens,
          });
        },
        scores: [
          // Factuality (LLM-as-judge)
          async (args: { input: any; output: string; expected?: string }) => {
            let outputText: string;
            try {
              const parsed = JSON.parse(args.output);
              outputText = parsed.finalText ?? args.output;
            } catch {
              outputText = args.output;
            }
            return Factuality({
              input: args.input.prompt,
              output: outputText,
              expected: args.expected,
            });
          },
          // Completeness scorer
          (args: { input: any; output: string; expected?: string }) => {
            const tc = testCases.find(
              (t) => t.id === args.input.testCaseId,
            ) as TestCase;
            const expected = tc.expected;
            let outputText: string;
            try {
              const parsed = JSON.parse(args.output);
              outputText =
                parsed.finalText + "\n" + JSON.stringify(parsed.toolCalls);
            } catch {
              outputText = args.output;
            }
            return {
              name: "Completeness",
              score: scoreCompleteness(outputText, expected),
            };
          },
          // Efficiency scorer
          (args: { input: any; output: string; expected?: string }) => {
            let turnCount = 50;
            let totalTokens = 500_000;
            try {
              const parsed = JSON.parse(args.output);
              turnCount = parsed.turnCount ?? 50;
              totalTokens = parsed.totalTokens ?? 500_000;
            } catch {
              // Use worst-case defaults
            }
            return {
              name: "Efficiency",
              score: scoreEfficiency({ turnCount, totalTokens }),
            };
          },
        ],
      });
    }
  }
}
